{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification using XGBoost algorithm on SageMaker\n",
    "\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data ingestion](#Data-ingestion)\n",
    "3. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "4. [Hosting the model](#Hosting-model)\n",
    "   1. [Create model](#Create-model)\n",
    "   2. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "   3. [Create endpoint](#Create-endpoint)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "This notebook uses XGBoost algorithm to train MNIST dataset. The model is hosted on SageMaker endpoint. MNIST dataset has a total of 70,000 images. It's splitted to a training set of 60,000 examples and a test set of 10,000 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update SageMaker\n",
    "! pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket where the original mnist data is downloaded and stored.\n",
    "downloaded_data_bucket = f\"sagemaker-sample-files\"\n",
    "downloaded_data_prefix = \"datasets/image/MNIST\"\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "prefix = \"sagemaker/MNIST-xgboost-multiclass-classification\"\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = f\"s3://{bucket}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Download data from S3 public bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, json\n",
    "\n",
    "# Load the dataset\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(downloaded_data_bucket, f\"{downloaded_data_prefix}/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing\n",
    "\n",
    "The xgboost model consumes the libsvm converted data from S3 for training. The data needs to be converted from pickle-ized numpy array to the libsvm format before being uploaded to S3.  The following provides functions for data conversions and file upload to S3 and download from S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "\n",
    "def to_libsvm(f, labels, values):\n",
    "    f.write(\n",
    "        bytes(\n",
    "            \"\\n\".join(\n",
    "                [\n",
    "                    \"{} {}\".format(\n",
    "                        label, \" \".join([\"{}:{}\".format(i + 1, el) for i, el in enumerate(vec)])\n",
    "                    )\n",
    "                    for label, vec in zip(labels, values)\n",
    "                ]\n",
    "            ),\n",
    "            \"utf-8\",\n",
    "        )\n",
    "    )\n",
    "    return f\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region)\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(bucket)\n",
    "        .Object(key)\n",
    "        .upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    import pickle\n",
    "    import gzip\n",
    "\n",
    "    with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = \"latin1\"\n",
    "        return u.load()\n",
    "\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "    labels = [t.tolist() for t in partition[1]]\n",
    "    vectors = [t.tolist() for t in partition[0]]\n",
    "    num_partition = 5  # partition file into 5 parts\n",
    "    partition_bound = int(len(labels) / num_partition)\n",
    "    for i in range(num_partition):\n",
    "        f = io.BytesIO()\n",
    "        to_libsvm(\n",
    "            f,\n",
    "            labels[i * partition_bound : (i + 1) * partition_bound],\n",
    "            vectors[i * partition_bound : (i + 1) * partition_bound],\n",
    "        )\n",
    "        f.seek(0)\n",
    "        key = f\"{prefix}/{partition_name}/examples{str(i)}\"\n",
    "        url = f\"s3://{bucket}/{key}\"\n",
    "        print(f\"Writing to {url}\")\n",
    "        write_to_s3(f, bucket, key)\n",
    "        print(f\"Done writing to {url}\")\n",
    "\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = f\"{prefix}/{partition_name}/examples{number}\"\n",
    "    url = f\"s3://{bucket}/{key}\"\n",
    "    print(f\"Reading from {url}\")\n",
    "    s3 = boto3.resource(\"s3\", region_name=region)\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(key, \"mnist.local.test\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            print(f\"The object does not exist at {url}.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def convert_data():\n",
    "    train_set, valid_set, test_set = get_dataset()\n",
    "    partitions = [(\"train\", train_set), (\"validation\", valid_set), (\"test\", test_set)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print(f\"{partition_name}: {partition[0].shape} {partition[1].shape}\")\n",
    "        upload_to_s3(partition_name, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "convert_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n",
    "\n",
    "Now that we have our data in S3, we can begin training. We'll use Amazon SageMaker XGboost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the train and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "common_training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"File\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": f\"{bucket_path}/{prefix}/xgboost\"},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m4.10xlarge\", \"VolumeSizeInGB\": 5},\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\": \"5\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"verbosity\": \"0\",\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": \"10\",\n",
    "        \"num_round\": \"10\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{bucket_path}/{prefix}/train/\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f\"{bucket_path}/{prefix}/validation/\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a single instance trainng job\n",
    "\n",
    "### Training on a single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single machine job params\n",
    "single_machine_job_name = f'DEMO-xgboost-classification{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "print(\"Job name is:\", single_machine_job_name)\n",
    "\n",
    "single_machine_job_params = copy.deepcopy(common_training_params)\n",
    "single_machine_job_params[\"TrainingJobName\"] = single_machine_job_name\n",
    "single_machine_job_params[\"OutputDataConfig\"][\n",
    "    \"S3OutputPath\"\n",
    "] = f\"{bucket_path}/{prefix}/xgboost-single\"\n",
    "single_machine_job_params[\"ResourceConfig\"][\"InstanceCount\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm = boto3.Session(region_name=region).client(\"sagemaker\")\n",
    "\n",
    "sm.create_training_job(**single_machine_job_params)\n",
    "#sm.create_training_job(**distributed_job_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=single_machine_job_name)[\"TrainingJobStatus\"]\n",
    "print(status)\n",
    "sm.get_waiter(\"training_job_completed_or_stopped\").wait(TrainingJobName=single_machine_job_name)\n",
    "\n",
    "\n",
    "if status == \"Failed\":\n",
    "    message = sm.describe_training_job(TrainingJobName=single_machine_job_name)[\"FailureReason\"]\n",
    "    print(f\"Training failed with the following error: {message}\")\n",
    "    raise Exception(\"Training job failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm both jobs have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Single Machine:\",\n",
    "    sm.describe_training_job(TrainingJobName=single_machine_job_name)[\"TrainingJobStatus\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting model\n",
    "In order to set up hosting, we will create model, create endpoint configuration, and then create endpoint. The step below demonstrated hosting the single machine job. \n",
    "\n",
    "\n",
    "### Create model\n",
    "Next, you register the model with hosting. This allows you the flexibility of importing models trained elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{single_machine_job_name}-model\"\n",
    "print(model_name)\n",
    "\n",
    "info = sm.describe_training_job(TrainingJobName=single_machine_job_name)\n",
    "\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\"Image\": container, \"ModelDataUrl\": model_data}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "SageMaker supports configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, customers create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way. In addition, the endpoint configuration describes the instance type required for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = f'DEMO-XGBoostEndpointConfig-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f'Endpoint Config Arn: {create_endpoint_config_response[\"EndpointConfigArn\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 9-11 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = f'DEMO-XGBoostEndpoint-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(f\"Status: {status}\")\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(f\"Status: {status}\")\n",
    "\n",
    "print(f'Arn: {resp[\"EndpointArn\"]}')\n",
    "print(f\"Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Finally, the customer can now validate the model for use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client(\"runtime.sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the model, we'll use the test dataset previously generated. Let us first download the data from S3 to the local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_s3(\"test\", 0, \"mnist.local.test\")  # reading the first part file within test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a single prediction. Lets use the first record from the test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 mnist.local.test > mnist.single.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = (\n",
    "    \"mnist.single.test\"  # customize to your test file 'mnist.single.test' if use the data above\n",
    ")\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"text/x-libsvm\", Body=payload\n",
    ")\n",
    "result = response[\"Body\"].read().decode(\"ascii\")\n",
    "print(f\"Predicted label is {result}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, a single prediction works.\n",
    "Let's do a whole batch and see how good is the predictions accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = \"\\n\".join(data)\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=payload\n",
    "    )\n",
    "    result = response[\"Body\"].read().decode(\"ascii\")\n",
    "    preds = [float(num) for num in result.split(\"\\n\")[:-1]]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    for offset in range(0, items, batch_size):\n",
    "        arrs.extend(\n",
    "            do_predict(data[offset : min(offset + batch_size, items)], endpoint_name, content_type)\n",
    "        )\n",
    "        sys.stdout.write(\".\")\n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps us calculate the error rate on the batch dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = \"mnist.local.test\"\n",
    "with open(file_name, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [float(line.split(\" \")[0]) for line in payload.split(\"\\n\")]\n",
    "test_data = payload.split(\"\\n\")\n",
    "preds = batch_predict(test_data, 100, endpoint_name, \"text/x-libsvm\")\n",
    "\n",
    "print(\n",
    "    \"\\nerror rate=%f\"\n",
    "    % (sum(1 for i in range(len(preds)) if preds[i] != labels[i]) / float(len(preds)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helps us visualize the erros that the XGBoost classifier is making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n",
    "Once you are done using the endpoint, you can use the following to delete it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
